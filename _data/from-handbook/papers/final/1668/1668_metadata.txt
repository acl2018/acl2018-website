SubmissionNumber#=%=#1668
FinalPaperTitle#=%=#Are BLEU and Meaning Representation in Opposition?
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Ondřej Cífka
JobTitle#==#
Organization#==#Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
Abstract#==#One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.
Author{1}{Firstname}#=%=#Ondřej
Author{1}{Lastname}#=%=#Cífka
Author{1}{Email}#=%=#ondra@cifka.com
Author{1}{Affiliation}#=%=#Charles University, Faculty of Mathematics and Physics
Author{2}{Firstname}#=%=#Ondřej
Author{2}{Lastname}#=%=#Bojar
Author{2}{Email}#=%=#bojar@ufal.mff.cuni.cz
Author{2}{Affiliation}#=%=#Charles University in Prague, Faculty of Mathematics and Physics

==========