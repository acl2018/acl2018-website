SubmissionNumber#=%=#187
FinalPaperTitle#=%=#Probabilistic FastText for Multi-Sense Word Embeddings
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Ben Athiwaratkun
JobTitle#==#
Organization#==#
Abstract#==#We introduce Probabilistic FastText, a new model for word embeddings that can
capture multiple word senses, sub-word structure, and uncertainty information. 
In particular, we represent each word with a Gaussian mixture density, where
the mean of a mixture component is given by the sum of n-grams.  This
representation allows the model to share the ``strength'' across sub-word
structures (e.g. Latin roots), producing accurate representations of rare,
misspelt, or even unseen words. Moreover, each component of the mixture can
capture a different word sense. Probabilistic FastText outperforms both
FastText, which has no probabilistic model, and dictionary-level probabilistic
embeddings, which do not incorporate subword structures, on several
word-similarity benchmarks, including English RareWord and foreign language
datasets.
We also achieve state-of-art performance on benchmarks that measure ability to
discern different meanings. 
Thus, our model is the first to achieve best of both the worlds: multi-sense
representations while having enriched semantics on rare words.
Author{1}{Firstname}#=%=#Ben
Author{1}{Lastname}#=%=#Athiwaratkun
Author{1}{Email}#=%=#pa338@cornell.edu
Author{1}{Affiliation}#=%=#Cornell University
Author{2}{Firstname}#=%=#Andrew
Author{2}{Lastname}#=%=#Wilson
Author{2}{Email}#=%=#andrew@cornell.edu
Author{2}{Affiliation}#=%=#Cornell University
Author{3}{Firstname}#=%=#Anima
Author{3}{Lastname}#=%=#Anandkumar
Author{3}{Email}#=%=#anima@amazon.com
Author{3}{Affiliation}#=%=#Amazon, Caltech

==========