SubmissionNumber#=%=#235
FinalPaperTitle#=%=#Fluency Boost Learning and Inference for Neural Grammatical Error Correction
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Tao Ge
JobTitle#==#Researcher
Organization#==#Microsoft Research Asia, Beijing, China
Abstract#==#Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence's fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence's fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.
Author{1}{Firstname}#=%=#Tao
Author{1}{Lastname}#=%=#Ge
Author{1}{Email}#=%=#tage@microsoft.com
Author{1}{Affiliation}#=%=#Microsoft Research Asia
Author{2}{Firstname}#=%=#Furu
Author{2}{Lastname}#=%=#Wei
Author{2}{Email}#=%=#fuwei@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft Research Asia
Author{3}{Firstname}#=%=#Ming
Author{3}{Lastname}#=%=#Zhou
Author{3}{Email}#=%=#mingzhou@microsoft.com
Author{3}{Affiliation}#=%=#microsoft research asia

==========