SubmissionNumber#=%=#1547
FinalPaperTitle#=%=#Token-level and sequence-level loss smoothing for RNN language models
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Maha Elbayad
JobTitle#==#
Organization#==#Inria,  655 Avenue de l'Europe, 38330 Montbonnot-Saint-Martin, France
Abstract#==#Despite the effectiveness of recurrent neural network language models, their 
maximum likelihood estimation suffers from two limitations. It treats all
sentences that do not match the ground truth as equally poor, ignoring the
structure of the output space. 
Second, it suffers from 'exposure bias': during training tokens are predicted
given ground-truth sequences, while at test time prediction is conditioned on
generated output sequences.
To overcome these limitations 
we build upon the recent reward augmented maximum likelihood  approach that
encourages the model to predict sentences that are close to the ground truth
according to a given performance metric.
We extend this approach to token-level loss smoothing, and propose improvements
to the sequence-level smoothing approach. 
Our experiments on two different tasks, image captioning and machine
translation,  show that token-level and sequence-level loss smoothing are
complementary, and significantly improve results.
Author{1}{Firstname}#=%=#Maha
Author{1}{Lastname}#=%=#ELBAYAD
Author{1}{Email}#=%=#maha.elbayad@inria.fr
Author{1}{Affiliation}#=%=#INRIA / LIG
Author{2}{Firstname}#=%=#Laurent
Author{2}{Lastname}#=%=#Besacier
Author{2}{Email}#=%=#laurent.besacier@imag.fr
Author{2}{Affiliation}#=%=#LIG
Author{3}{Firstname}#=%=#Jakob
Author{3}{Lastname}#=%=#Verbeek
Author{3}{Email}#=%=#jakob.verbeek@inria.fr
Author{3}{Affiliation}#=%=#INRIA

==========