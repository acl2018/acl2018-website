SubmissionNumber#=%=#1414
FinalPaperTitle#=%=#Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Poorya Zaremoodi
JobTitle#==#
Organization#==#Monash University
Abstract#==#Neural Machine Translation (NMT) is notorious for its need for large amounts of
bilingual data. An effective approach to compensate for this requirement is
Multi-Task Learning (MTL) to leverage different linguistic resources as a
source of inductive bias. Current MTL architectures are based on the Seq2Seq
transduction, and (partially) share different components of the models among
the tasks. However, this MTL approach often suffers from task interference and
is not able to fully capture commonalities among subsets of tasks.
We address this issue by extending the recurrent units with multiple "blocks"
along with a trainable "routing network". The routing network enables adaptive
collaboration by dynamic  sharing of blocks conditioned on the task at hand, 
input, and model state. Empirical evaluation of two low-resource translation
tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements
compared to strong baselines.
Author{1}{Firstname}#=%=#Poorya
Author{1}{Lastname}#=%=#Zaremoodi
Author{1}{Email}#=%=#poorya.zaremoodi@monash.edu
Author{1}{Affiliation}#=%=#Monash University
Author{2}{Firstname}#=%=#Wray
Author{2}{Lastname}#=%=#Buntine
Author{2}{Email}#=%=#wray.buntine@monash.edu
Author{2}{Affiliation}#=%=#Monash University
Author{3}{Firstname}#=%=#Gholamreza
Author{3}{Lastname}#=%=#Haffari
Author{3}{Email}#=%=#reza.haffari@gmail.com
Author{3}{Affiliation}#=%=#Monash University

==========