SubmissionNumber#=%=#542
FinalPaperTitle#=%=#Simple and Effective Multi-Paragraph Reading Comprehension
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Christopher Clark
JobTitle#==#
Organization#==#Allen Institute for Artificial Intelligence
Abstract#==#We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.
Author{1}{Firstname}#=%=#Christopher
Author{1}{Lastname}#=%=#Clark
Author{1}{Email}#=%=#csquared@cs.washington.edu
Author{1}{Affiliation}#=%=#University of Washington
Author{2}{Firstname}#=%=#Matt
Author{2}{Lastname}#=%=#Gardner
Author{2}{Email}#=%=#mattg@allenai.org
Author{2}{Affiliation}#=%=#Allen Institute for Artificial Intelligence

==========