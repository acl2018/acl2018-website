SubmissionNumber#=%=#1472
FinalPaperTitle#=%=#How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Tobias Domhan
JobTitle#==#Machine Learning Scientist
Organization#==#Amazon Development Center Germany, Krausenstrasse 38, 10117 Berlin, Germany
Abstract#==#With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.
Author{1}{Firstname}#=%=#Tobias
Author{1}{Lastname}#=%=#Domhan
Author{1}{Email}#=%=#domhant@amazon.de
Author{1}{Affiliation}#=%=#Amazon

==========