SubmissionNumber#=%=#1017
FinalPaperTitle#=%=#Distilling Knowledge for Search-based Structured Prediction
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Yijia Liu
JobTitle#==#
Organization#==#Harbin Institute of Technology
Abstract#==#Many natural language processing tasks can be modeled into 
structured prediction and solved as a  search problem.
In this paper, we distill an ensemble of multiple
models trained with different initialization into a single model. 
In addition to learning to
match the ensemble's probability output on the reference states,
we also
use the ensemble to explore the search space and learn from
the encountered states in the exploration. 
Experimental results on
two typical search-based structured prediction tasks --
transition-based dependency parsing and neural machine translation show that
distillation can effectively improve the single model's performance
and
the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score
on these two tasks respectively over
strong baselines and it outperforms the greedy structured prediction models
in previous literatures.
Author{1}{Firstname}#=%=#Yijia
Author{1}{Lastname}#=%=#Liu
Author{1}{Email}#=%=#oneplus.lau@gmail.com
Author{1}{Affiliation}#=%=#Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology
Author{2}{Firstname}#=%=#Wanxiang
Author{2}{Lastname}#=%=#Che
Author{2}{Email}#=%=#wanxiang@gmail.com
Author{2}{Affiliation}#=%=#Harbin Institute of Technology
Author{3}{Firstname}#=%=#Huaipeng
Author{3}{Lastname}#=%=#Zhao
Author{3}{Email}#=%=#huaipengzhao@gmail.com
Author{3}{Affiliation}#=%=#Harbin Institute of Technology
Author{4}{Firstname}#=%=#Bing
Author{4}{Lastname}#=%=#Qin
Author{4}{Email}#=%=#qinb@ir.hit.edu.cn
Author{4}{Affiliation}#=%=#Harbin Institute of Technology
Author{5}{Firstname}#=%=#Ting
Author{5}{Lastname}#=%=#Liu
Author{5}{Email}#=%=#tliu72@vip.126.com
Author{5}{Affiliation}#=%=#Harbin Institute of Technology

==========