SubmissionNumber#=%=#574
FinalPaperTitle#=%=#Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Rui Wang
JobTitle#==#
Organization#==#NICT
Abstract#==#Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.
Author{1}{Firstname}#=%=#Rui
Author{1}{Lastname}#=%=#Wang
Author{1}{Email}#=%=#wangrui@nict.go.jp
Author{1}{Affiliation}#=%=#NICT
Author{2}{Firstname}#=%=#Masao
Author{2}{Lastname}#=%=#Utiyama
Author{2}{Email}#=%=#mutiyama@nict.go.jp
Author{2}{Affiliation}#=%=#NICT
Author{3}{Firstname}#=%=#Eiichiro
Author{3}{Lastname}#=%=#Sumita
Author{3}{Email}#=%=#eiichiro.sumita@nict.go.jp
Author{3}{Affiliation}#=%=#NICT

==========